{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1568b052522aa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a643abd777d646fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergSparkSession\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.demo.uri\", \"http://rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://demo/warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6848cc5db645e421",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.sql.\n: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3://warehouse/nyc/taxis/metadata/00000-77385378-e410-4cd1-b52f-a527eac49ca3.metadata.json\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:465)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:489)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)\n\tat org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mCREATE TABLE demo.nyc.taxis (\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m  vendor_id BIGINT,\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m  trip_id BIGINT,\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m  trip_distance FLOAT,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m  fare_amount DOUBLE,\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m  store_and_fwd_flag STRING\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mPARTITIONED BY (vendor_id)\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.sql.\n: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3://warehouse/nyc/taxis/metadata/00000-77385378-e410-4cd1-b52f-a527eac49ca3.metadata.json\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:465)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:489)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)\n\tat org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE demo.nyc.taxis (\n",
    "  vendor_id BIGINT,\n",
    "  trip_id BIGINT,\n",
    "  trip_distance FLOAT,\n",
    "  fare_amount DOUBLE,\n",
    "  store_and_fwd_flag STRING\n",
    ")\n",
    "PARTITIONED BY (vendor_id)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6f5c1f17a18c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/26 15:28:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Iceberg table 'demo.nyc.taxis' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session with Iceberg + MinIO config\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergSparkSession\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.demo.uri\", \"http://rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://demo/warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create namespace (database) if not exists\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.nyc\")\n",
    "\n",
    "# Create Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE demo.nyc.taxis (\n",
    "  vendor_id BIGINT,\n",
    "  trip_id BIGINT,\n",
    "  trip_distance FLOAT,\n",
    "  fare_amount DOUBLE,\n",
    "  store_and_fwd_flag STRING\n",
    ")\n",
    "USING ICEBERG\n",
    "PARTITIONED BY (vendor_id)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Iceberg table 'demo.nyc.taxis' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8af550af8b7be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO demo.nyc.taxis\n",
    "VALUES\n",
    "  (1, 1000371, 1.8, 15.32, 'N'),\n",
    "  (2, 1000372, 2.5, 22.15, 'N'),\n",
    "  (2, 1000373, 0.9, 9.01, 'N'),\n",
    "  (1, 1000374, 8.4, 42.13, 'Y')\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc075b1c-d847-4378-b540-177ec724ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>(150 + 1) / 151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV loaded in 80.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv = spark.read.csv(\"/home/iceberg/data/dduplicated_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ CSV loaded in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924f25b7-e5ce-49a0-b3ac-4b58ebbe78c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV loaded in 0.36 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "parquet = spark.read.parquet(\"/home/iceberg/data/ddf_duplicated.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ CSV loaded in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927bff85-de3c-4922-8833-035b5fd72b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>  (145 + 6) / 151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV loaded in 51.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "csv.count()\n",
    "end_time = time.time()\n",
    "print(f\"✅ CSV loaded in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db365846-172b-49c9-9068-200daeb52c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV loaded in 0.37 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "parquet.count()\n",
    "end_time = time.time()\n",
    "print(f\"✅ CSV loaded in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209059d8-bc0f-481c-9c19-74ec189b96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 16:02:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Iceberg Spark + REST\")\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"rest\")\n",
    "    .config(\"spark.sql.catalog.demo.uri\", \"http://rest:8181\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://warehouse/warehouse/\")\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    \n",
    "    # MinIO S3FileIO settings (match REST catalog)\n",
    "    .config(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.demo.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.demo.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.demo.s3.secret-access-key\", \"password\")\n",
    "    \n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e65a3b7-0b5b-4c96-a669-ab017fd1426e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3371178588.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    CALL demo.system.register_table(\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CALL demo.system.register_table(\n",
    "  'source.auchan',\n",
    "  's3://warehouse/warehouse/source/auchan/metadata/v1.metadata.json'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9976aa-30fe-4db6-9e81-9722a17f5e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.source\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f590714-b47c-44f1-9c69-9f98c636fd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.auchan (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    price STRING,\n",
    "    promotion STRING,\n",
    "    quantity_stock STRING,\n",
    "    category STRING,\n",
    "    sub_category STRING,\n",
    "    quantity STRING,\n",
    "    price_per_quantity STRING,\n",
    "    marque STRING,\n",
    "    date STRING,\n",
    "    image_url STRING,\n",
    "    store STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f639574-2a61-4850-a548-55636b07b3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://warehouse/warehouse/source/auchan\n"
     ]
    }
   ],
   "source": [
    "desc = spark.sql(\"DESCRIBE TABLE EXTENDED demo.source.auchan\").collect()\n",
    "for row in desc:\n",
    "    if row.col_name == \"Location\":\n",
    "        print(row.data_type)  # This prints the table location URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9779df6a-74b6-4273-9bb3-faaa8a40cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|   source|            auchan|      false|\n",
      "|   source|        auchan_cat|      false|\n",
      "|   source|       auchan_norm|      false|\n",
      "|   source|    auchan_section|      false|\n",
      "|   source|           biocoop|      false|\n",
      "|   source|       biocoop_cat|      false|\n",
      "|   source|   biocoop_section|      false|\n",
      "|   source|         carrefour|      false|\n",
      "|   source|     carrefour_cat|      false|\n",
      "|   source| carrefour_section|      false|\n",
      "|   source|laballevie_section|      false|\n",
      "|   source|        labellevie|      false|\n",
      "|   source|    labellevie_cat|      false|\n",
      "|   source|   labellevie_norm|      false|\n",
      "|   source|labellevie_section|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN demo.source\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e01c6c-e51a-4f18-bf93-0608d5933674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750279c2-ea3c-4e2e-832c-d53a16527254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------+\n",
      "|info_name     |info_value                     |\n",
      "+--------------+-------------------------------+\n",
      "|Catalog Name  |demo                           |\n",
      "|Namespace Name|source                         |\n",
      "|Location      |s3://warehouse/warehouse/source|\n",
      "|Owner         |root                           |\n",
      "|Properties    |                               |\n",
      "+--------------+-------------------------------+\n",
      "\n",
      "+------------------+---------+-------+\n",
      "|col_name          |data_type|comment|\n",
      "+------------------+---------+-------+\n",
      "|id                |int      |NULL   |\n",
      "|name              |string   |NULL   |\n",
      "|price             |string   |NULL   |\n",
      "|promotion         |string   |NULL   |\n",
      "|quantity_stock    |string   |NULL   |\n",
      "|category          |string   |NULL   |\n",
      "|sub_category      |string   |NULL   |\n",
      "|quantity          |string   |NULL   |\n",
      "|price_per_quantity|string   |NULL   |\n",
      "|marque            |string   |NULL   |\n",
      "|date              |string   |NULL   |\n",
      "|image_url         |string   |NULL   |\n",
      "|store             |string   |NULL   |\n",
      "|                  |         |       |\n",
      "|# Metadata Columns|         |       |\n",
      "|_spec_id          |int      |       |\n",
      "|_partition        |struct<> |       |\n",
      "|_file             |string   |       |\n",
      "|_pos              |bigint   |       |\n",
      "|_deleted          |boolean  |       |\n",
      "+------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "spark.sql.catalog.demo.s3.endpoint = http://minio:9000\n",
      "spark.sql.catalog.demo.warehouse = s3://warehouse/wh/\n",
      "spark.sql.catalog.demo.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalog.demo.uri = http://rest:8181\n",
      "spark.sql.catalog.demo.type = rest\n",
      "spark.sql.catalog.demo = org.apache.iceberg.spark.SparkCatalog\n"
     ]
    }
   ],
   "source": [
    "# Check what storage properties Spark sees for the catalog\n",
    "spark.sql(\"DESCRIBE NAMESPACE EXTENDED demo.source\").show(truncate=False)\n",
    "\n",
    "# Check table properties to see storage locations\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED demo.source.auchan\").show(truncate=False)\n",
    "\n",
    "# Show catalog configuration\n",
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    if 'catalog' in conf[0].lower() and 'demo' in conf[0]:\n",
    "        print(f\"{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb21b9d-b588-4c94-a505-5ebe16f9f1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                              |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                                                    |NULL   |\n",
      "|name                        |string                                                                                                 |NULL   |\n",
      "|price                       |string                                                                                                 |NULL   |\n",
      "|promotion                   |string                                                                                                 |NULL   |\n",
      "|quantity_stock              |string                                                                                                 |NULL   |\n",
      "|category                    |string                                                                                                 |NULL   |\n",
      "|sub_category                |string                                                                                                 |NULL   |\n",
      "|quantity                    |string                                                                                                 |NULL   |\n",
      "|price_per_quantity          |string                                                                                                 |NULL   |\n",
      "|marque                      |string                                                                                                 |NULL   |\n",
      "|date                        |string                                                                                                 |NULL   |\n",
      "|image_url                   |string                                                                                                 |NULL   |\n",
      "|store                       |string                                                                                                 |NULL   |\n",
      "|                            |                                                                                                       |       |\n",
      "|# Metadata Columns          |                                                                                                       |       |\n",
      "|_spec_id                    |int                                                                                                    |       |\n",
      "|_partition                  |struct<>                                                                                               |       |\n",
      "|_file                       |string                                                                                                 |       |\n",
      "|_pos                        |bigint                                                                                                 |       |\n",
      "|_deleted                    |boolean                                                                                                |       |\n",
      "|                            |                                                                                                       |       |\n",
      "|# Detailed Table Information|                                                                                                       |       |\n",
      "|Name                        |demo.source.auchan                                                                                     |       |\n",
      "|Type                        |MANAGED                                                                                                |       |\n",
      "|Location                    |s3://warehouse/warehouse/source/auchan                                                                 |       |\n",
      "|Provider                    |iceberg                                                                                                |       |\n",
      "|Owner                       |root                                                                                                   |       |\n",
      "|Table Properties            |[current-snapshot-id=none,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd]|       |\n",
      "+----------------------------+-------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the complete table details including storage location\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED demo.source.auchan\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83c074d-0102-49ca-bf2a-33873a52bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo catalog URI: http://rest:8181\n",
      "Demo catalog type: rest\n",
      "spark.sql.catalog.demo.s3.endpoint = http://minio:9000\n",
      "spark.sql.catalogImplementation = in-memory\n",
      "spark.sql.catalog.demo.warehouse = s3://warehouse/wh/\n",
      "spark.sql.catalog.demo.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalog.demo.uri = http://rest:8181\n",
      "spark.sql.catalog.demo.type = rest\n",
      "spark.sql.catalog.demo = org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.defaultCatalog = demo\n"
     ]
    }
   ],
   "source": [
    "print(\"Demo catalog URI:\", spark.conf.get(\"spark.sql.catalog.demo.uri\"))\n",
    "print(\"Demo catalog type:\", spark.conf.get(\"spark.sql.catalog.demo.type\"))\n",
    "\n",
    "# List ALL Spark configurations to see catalog setup\n",
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    if 'catalog' in conf[0].lower():\n",
    "        print(f\"{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9d311d-d2f7-436a-a778-962b161cbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.sql.catalog.demo.s3.endpoint': 'http://minio:9000',\n",
       " 'spark.sql.catalogImplementation': 'in-memory',\n",
       " 'spark.sql.catalog.demo.warehouse': 's3://warehouse/wh/',\n",
       " 'spark.sql.catalog.demo.io-impl': 'org.apache.iceberg.aws.s3.S3FileIO',\n",
       " 'spark.sql.catalog.demo.uri': 'http://rest:8181',\n",
       " 'spark.sql.catalog.demo.type': 'rest',\n",
       " 'spark.sql.catalog.demo': 'org.apache.iceberg.spark.SparkCatalog'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in spark.sparkContext.getConf().getAll()\n",
    " if k.startswith(\"spark.sql.catalog\") or \"s3\" in k or \"fs.s3a\" in k}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "307066eb-60e8-4a21-a1f1-19022308f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.auchan_cat (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.biocoop_cat (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.carrefour_cat (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.labellevie_cat (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c47c8b2-1c02-43e2-851b-bf86ddbda549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.carrefour_section (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    section STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.auchan_section (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    section STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.biocoop_section (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    section STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.labellevie_section (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    section STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f66022b-c1c9-4bd8-bbd2-bf32e95f667f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.auchan_norm (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    u_quantity STRING,\n",
    "    unit STRING,\n",
    "    quantity STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ddfa4f7-7139-4662-86d9-95c2b6b3af32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.labellevie_norm (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  inside_parantheses STRING,\n",
    "  unit STRING,\n",
    "  quantity STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "055667cf-2f9e-4d08-98f1-f0ff93fb805a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.labellevie (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    price STRING,\n",
    "    promotion STRING,\n",
    "    quantity_stock STRING,\n",
    "    category STRING,\n",
    "    date TIMESTAMP,\n",
    "    expiration_date STRING,\n",
    "    image_url STRING,\n",
    "    store STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.carrefour (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    price STRING,\n",
    "    promotion STRING,\n",
    "    quantity_stock STRING,\n",
    "    price_per_quantity STRING,\n",
    "    date STRING,\n",
    "    expiration_date STRING,\n",
    "    image_url STRING,\n",
    "    store STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.source.biocoop (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    price STRING,\n",
    "    stock STRING,\n",
    "    price_per_quantity STRING,\n",
    "    date STRING,\n",
    "    img STRING,\n",
    "    store STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6480702f-70b9-4ca9-bc22-3188097c6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               name  price promotion  \\\n",
      "0  99085                          Batavia, Florette (200 g)  2.29€             \n",
      "1  99086                       Batavia en sachet, U (200 g)  2.06€             \n",
      "2  99087                   Laitue iceberg, Florette (175 g)  1.89€             \n",
      "3  99088                   Laitue iceberg, Florette (450 g)  3.17€             \n",
      "4  99089  Salade laitue iceberg sans résidu de pesticide...  2.36€             \n",
      "\n",
      "  quantity_stock               category       date  \\\n",
      "0              3  Les salades en sachet 2025-03-26   \n",
      "1             42  Les salades en sachet 2025-03-26   \n",
      "2             65  Les salades en sachet 2025-03-26   \n",
      "3             11  Les salades en sachet 2025-03-26   \n",
      "4              5  Les salades en sachet 2025-03-26   \n",
      "\n",
      "                                     expiration_date  \\\n",
      "0                   Batavia, Florette (200 g)\\n2.29€   \n",
      "1                Batavia en sachet, U (200 g)\\n2.06€   \n",
      "2            Laitue iceberg, Florette (175 g)\\n1.89€   \n",
      "3            Laitue iceberg, Florette (450 g)\\n3.17€   \n",
      "4  Salade laitue iceberg sans résidu de pesticide...   \n",
      "\n",
      "                                           image_url         store  \n",
      "0  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "1  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "2  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "3  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "4  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "      id                                               name  price promotion  \\\n",
      "0  99085                          Batavia, Florette (200 g)  2.29€             \n",
      "1  99086                       Batavia en sachet, U (200 g)  2.06€             \n",
      "2  99087                   Laitue iceberg, Florette (175 g)  1.89€             \n",
      "3  99088                   Laitue iceberg, Florette (450 g)  3.17€             \n",
      "4  99089  Salade laitue iceberg sans résidu de pesticide...  2.36€             \n",
      "\n",
      "  quantity_stock               category       date  \\\n",
      "0              3  Les salades en sachet 2025-03-26   \n",
      "1             42  Les salades en sachet 2025-03-26   \n",
      "2             65  Les salades en sachet 2025-03-26   \n",
      "3             11  Les salades en sachet 2025-03-26   \n",
      "4              5  Les salades en sachet 2025-03-26   \n",
      "\n",
      "                                     expiration_date  \\\n",
      "0                   Batavia, Florette (200 g)\\n2.29€   \n",
      "1                Batavia en sachet, U (200 g)\\n2.06€   \n",
      "2            Laitue iceberg, Florette (175 g)\\n1.89€   \n",
      "3            Laitue iceberg, Florette (450 g)\\n3.17€   \n",
      "4  Salade laitue iceberg sans résidu de pesticide...   \n",
      "\n",
      "                                           image_url         store  \n",
      "0  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "1  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "2  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "3  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n",
      "4  https://fridg-front.s3.amazonaws.com/media/CAC...  La belle vie  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/home/iceberg/data/labellevie.parquet\")\n",
    "print(df.head())\n",
    "# Convert timestamp columns\n",
    "for col in df.select_dtypes(include=[\"datetime64[ns]\"]).columns:\n",
    "    df[col] = df[col].dt.round(\"ms\")  # Drop nanosecond precision\n",
    "print(df.head())\n",
    "\n",
    "df.to_parquet(\"/home/iceberg/data/labellevie_fixed.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "013801d7-cd81-4f55-861b-5e0d1afec661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"/home/iceberg/data/labellevie.parquet\")\n",
    "\n",
    "# Convert timestamp columns to millisecond precision\n",
    "for col in df.select_dtypes(include=[\"datetime64[ns]\"]).columns:\n",
    "    df[col] = df[col].dt.round(\"ms\")\n",
    "\n",
    "# Write Parquet with millisecond precision (avoid TIMESTAMP(NANOS,false))\n",
    "df.to_parquet(\n",
    "    \"/home/iceberg/data/labellevie_fixed.parquet\",\n",
    "    index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    coerce_timestamps=\"ms\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e842875-82bc-4812-b19d-cb55b2457a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_labellevie = spark.read.parquet(\"/home/iceberg/data/labellevie_fixed.parquet\")\n",
    "df_labellevie.writeTo(\"demo.source.labellevie\").append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15a5ab07-6dd2-4d52-b866-ecbe826946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auchan = spark.read.parquet(\"/home/iceberg/data/auchan.parquet\", header=True, inferSchema=True)\n",
    "df_biocoop = spark.read.parquet(\"/home/iceberg/data/biocoop.parquet\", header=True, inferSchema=True)\n",
    "# df_labellevie = spark.read.parquet(\"/home/iceberg/data/labellevie.parquet\", header=True, inferSchema=True)\n",
    "df_carrefour = spark.read.parquet(\"/home/iceberg/data/carrefour.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "df_auchan_cat = spark.read.parquet(\"/home/iceberg/data/auchan_cat.parquet\", header=True, inferSchema=True)\n",
    "df_auchan_section = spark.read.parquet(\"/home/iceberg/data/auchan_section.parquet\", header=True, inferSchema=True)\n",
    "df_auchan_norm = spark.read.parquet(\"/home/iceberg/data/auchan_norm.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "df_labellevie_norm = spark.read.parquet(\"/home/iceberg/data/labellevie_norm.parquet\", header=True, inferSchema=True)\n",
    "df_labellevie_section = spark.read.parquet(\"/home/iceberg/data/labellevie_section.parquet\", header=True, inferSchema=True)\n",
    "df_labellevie_cat = spark.read.parquet(\"/home/iceberg/data/labellevie_cat.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "df_carrefour_cat = spark.read.parquet(\"/home/iceberg/data/carrefour_cat.parquet\", header=True, inferSchema=True)\n",
    "df_carrefour_section = spark.read.parquet(\"/home/iceberg/data/carrefour_section.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "df_biocoop_cat = spark.read.parquet(\"/home/iceberg/data/biocoop_cat.parquet\", header=True, inferSchema=True)\n",
    "df_biocoop_section = spark.read.parquet(\"/home/iceberg/data/biocoop_section.parquet\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39d85b65-9658-4935-ad6c-3c51907b1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labellevie_section = spark.read.parquet(\"/home/iceberg/data/Store_img.parquet\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26162c4d-64c1-4ba8-a06f-81b72230da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------+--------------+--------+------------+---------+------------------+--------------------+----------+--------------------+------+\n",
      "| id|                name|price|promotion|quantity_stock|category|sub_category| quantity|price_per_quantity|              marque|      date|           image_url| store|\n",
      "+---+--------------------+-----+---------+--------------+--------+------------+---------+------------------+--------------------+----------+--------------------+------+\n",
      "|  1|PLEINE FORME Œufs...|3,49€|     NULL|           101|      16|       oeufs|   6 œufs|       0,58€ / pce|        PLEINE FORME|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  2|LA NOUVELLE AGRIC...|4,79€|     NULL|            10|      16|       oeufs|  12 œufs|       0,40€ / pce|LA NOUVELLE AGRIC...|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  3|C'EST QUI LE PATR...|1,89€|     NULL|            13|      16|       oeufs|   6 œufs|       0,32€ / pce|C'EST QUI LE PATR...|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  4|BREIZH ON EGG Œuf...|4,05€|     NULL|            26|      16|       oeufs|  10 œufs|       0,41€ / pce|       BREIZH ON EGG|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  5|AUCHAN Oeufs de p...|4,09€|      5 %|            23|      16|       oeufs|12 pièces|       0,34€ / pce|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  6|MATINES Œufs de p...|3,25€|     NULL|            30|      16|       oeufs|  12 œufs|       0,27€ / pce|             MATINES|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  7|                NULL| NULL|     NULL|          NULL|      16|       oeufs|     NULL|              NULL|                NULL|2025-03-01|                NULL|Auchan|\n",
      "|  8|L'OEUF DE NOS VIL...|1,69€|     NULL|            28|      16|       oeufs|   6 œufs|       0,28€ / pce|L'OEUF DE NOS VIL...|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "|  9|AUCHAN Oeufs de p...|2,72€|      5 %|             3|      16|       oeufs|12 pièces|       0,23€ / pce|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 10|L'OEUF DE NOS VIL...|2,39€|     NULL|             7|      16|       oeufs|   6 œufs|       0,40€ / pce|L'OEUF DE NOS VIL...|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 11|AUCHAN Oeufs de p...|4,59€|      5 %|            15|      16|       oeufs|18 pièces|       0,26€ / pce|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 12|AUCHAN Oeufs de p...|2,11€|      5 %|            68|      16|       oeufs| 6 pièces|       0,35€ / pce|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 13|BREIZH ON EGG Œuf...|2,46€|     NULL|            14|      16|       oeufs|   6 œufs|       0,41€ / pce|       BREIZH ON EGG|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 14|AUCHAN Oeufs de p...|1,58€|     NULL|            83|      16|       oeufs| 6 pièces|       0,26€ / pce|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 15|PLEINE FORME Œufs...|4,75€|     NULL|            26|      16|       oeufs|  10 œufs|       0,48€ / pce|        PLEINE FORME|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 16|AUCHAN Beurre ten...|2,45€|      5 %|             6|      16|     beurres|     250g|        9,80€ / kg|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 17|ELLE & VIRE Mini-...|3,37€|     NULL|            14|      16|     beurres|     200g|       16 portions|         ELLE & VIRE|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 18|AUCHAN Beurre ten...|2,45€|      5 %|            53|      16|     beurres|     250g|        9,80€ / kg|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 19|  AUCHAN Beurre doux|2,49€|      5 %|            67|      16|     beurres|     250g|        9,96€ / kg|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "| 20|  AUCHAN Beurre doux|2,98€|      5 %|             7|      16|     beurres|     250g|       11,92€ / kg|              AUCHAN|2025-03-01|https://cdn.aucha...|Auchan|\n",
      "+---+--------------------+-----+---------+--------------+--------+------------+---------+------------------+--------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f9298f6-331a-4def-b41d-396ecf8b3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.writeTo(\"demo.source.auchan\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32937735-9372-40f6-9dde-3a9a6fa8dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "source|         auchan|      false|\n",
    "|   source|    auchan_norm|      false|\n",
    "|   source|        biocoop|      false|\n",
    "|   source|      carrefour|      false|\n",
    "|   source|     labellevie|      false|\n",
    "|   source|labellevie_norm|      false|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db13662c-fc9c-4295-9501-af8fe3869741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_auchan.writeTo(\"demo.source.auchan\").append()\n",
    "# df_biocoop.writeTo(\"demo.source.biocoop\").append()\n",
    "# df_labellevie.writeTo(\"demo.source.labellevie\").append()\n",
    "# df_carrefour.writeTo(\"demo.source.carrefour\").append()\n",
    "# df_auchan_cat.writeTo(\"demo.source.auchan_cat\").append()\n",
    "# df_auchan_section.writeTo(\"demo.source.auchan_section\").append()\n",
    "# df_auchan_norm.writeTo(\"demo.source.auchan_norm\").append()\n",
    "# df_labellevie_norm.writeTo(\"demo.source.labellevie_norm\").append()\n",
    "df_labellevie_section.writeTo(\"demo.source.labellevie_section\").append()\n",
    "# df_labellevie_cat.writeTo(\"demo.source.labellevie_cat\").append()\n",
    "# df_carrefour_cat.writeTo(\"demo.source.carrefour_cat\").append()\n",
    "# df_carrefour_section.writeTo(\"demo.source.carrefour_section\").append()\n",
    "# df_biocoop_cat.writeTo(\"demo.source.biocoop_cat\").append()\n",
    "# df_biocoop_section.writeTo(\"demo.source.biocoop_section\").append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "183e6218-705c-4bb1-8b3d-32a6a8c98df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labellevie_section.writeTo(\"demo.source.Store_img\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f371320e-2448-41c7-af87-031f5895a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|   source|         Store_img|      false|\n",
      "|   source|            auchan|      false|\n",
      "|   source|        auchan_cat|      false|\n",
      "|   source|       auchan_norm|      false|\n",
      "|   source|    auchan_section|      false|\n",
      "|   source|           biocoop|      false|\n",
      "|   source|       biocoop_cat|      false|\n",
      "|   source|   biocoop_section|      false|\n",
      "|   source|         carrefour|      false|\n",
      "|   source|     carrefour_cat|      false|\n",
      "|   source| carrefour_section|      false|\n",
      "|   source|laballevie_section|      false|\n",
      "|   source|        labellevie|      false|\n",
      "|   source|    labellevie_cat|      false|\n",
      "|   source|   labellevie_norm|      false|\n",
      "|   source|labellevie_section|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN demo.source\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375c768-4eb4-434e-965a-83f930f488d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
